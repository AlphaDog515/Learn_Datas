一、HadoopHA

1. HadoopHA主要是为了消除单点故障，我要在集群中启动备用的NN,RM

2. 在配置时，需要同时启动多个NN和RM
	在启动时，以HDFS的HA为例！
	
	①多个NN，使用状态对NN进行标识，只有active的NN可以处理客户端请求，为主机！
		standby状态的NN作为备机，standby状态的NN需要及时同步主机的元数据，保证元数据一致
			
	②在第一次启动NN时，使用同步命令，将格式化后的fsimage文件，同步到所有的NN
		在activeNN运行期间，使用JournalNode来同步edits文件！
		activeNN将edits文件，同步到JournalNode，再由standby状态的NN从JournalNode上同步！
		
		JournalNode基于Paxos协议设计，适合在奇数台机器运行，最少需要3个！
		
		启用了HDFS 的HA，不能再启动2NN！
			
	③在启动后，需要手动将其中的一个NN提升为active状态
		使用  hdfs  haadmin -trasitionToActive  xxx
			
		在切换时，必须保证当前集群中没有为active状态的NN！
	  
		在HDFS的HA中，必须避免脑裂(集群中出现了多个active状态的NN)
		  
		 
3. 自动故障转移
	借助Zookeeper实现自动的故障转移！
	
	在每个NN所在的机器，启动一个ZKFC进程，这个进程会维护一个zookeeper客户端！
	在启动了这个进程后，如果当前机器的NN可用，ZKFC进程会抢先向ZK集群中注册一个临时节点！
	哪个ZKFC进程拥有对这个临时节点的所有权，会对节点加锁！在此进程的Session未断开前，占用此节点！
	此时，这个ZKFC进程会将当前机器的NN提升为active!
	
	每个ZKFC进程负责和NN进行进行ping通信，一旦发现ping命令回复超时，会认为此NN已经假死，
	假死后，zkfc会断开session，放弃对zookeeper上节点的控制权，其他的ZKFC进程负责监听节点，
	一旦节点不存在，其他的zkfc进行开始抢占此节点，哪个ZKFC进程抢占成功，
	就会把它所在机器的NN提升为active，在提升之前，必须保证不能出现脑裂，
	因此使用fence机制，将之前的NN进行隔离！

	
二、压缩

1.压缩的目的是在MR运行期间减少磁盘IO和网络IO

2. 压缩的场景
	IO密集型，可以多用压缩！
	运行密集型，CPU负载过重，少用或不用压缩！
		
3. 在MR中压缩的位置
	输入的文件采用压缩格式：  
		当单个文件压缩之后，如果超过130M，可以考虑使用带切片的压缩格式！
	
	在shuffle阶段使用压缩：
		考虑速度！速度快即可！
	
	reduce输出的结果采用压缩：
		考虑当前Job的输出是否还会作为下一个Job的输入，再考虑输出的结果每个大小是否会超过130M,
		如果超过，还需要作为下一个Job的输入，可以考虑使用带切片的压缩格式！
				
4. 常见的压缩格式
	Gzip,Deflate:  	压缩比尚可，速度中规中矩，默认自带
	
	Bzip2:   		压缩比高，速度慢，可切片
					一般作为输入和输出！
			
	LZO：  			需要额外安装，使用麻烦(设置索引，设置输入格式)。
					速度较快，可切片			
					一般作为输入和输出！
			
	Snappy：  		速度最快，压缩比凑合，使用简单	
					一般作为shuffle阶段使用的压缩格式！
				
5. 使用JavaAPI执行压缩和解压缩		
	CompressionCodec: 代表压缩个编码和解码格式！
		getCompressionInputStream: 获取一个可以解压缩的输入流
		getCompressionOutputStream: 获取一个可以压缩的输出流

		
三、调度器
hadoop中提供了三种调度器
	FIFO: 	单队列，先提交先服务。
			如果队列中有一个大的Job，大的Job后续的Job都需要等待，排队！
			集群的资源利用率低！
	
	容量调度器：
		多个队列，先提交先服务。
		每个队列可以设置占用集群总资源的一定容量！在队列资源空闲时，
		可以让空闲资源暂时提供给其他队列使用！
		每个用户可以设置占用队列资源的一定容量！
		每个Job可以设置占用队列资源的一定容量！
		配置灵活，在线刷新即可！
		
		
	公平调度器：
		配置和设计上和容量调度器一致！
		区别在调度策略上，采用最大最小公平算法，基于内存进行计算，
		每个队列中已经提交的Job可以平等第获取队列中的资源！ 
		造成小的Job可以及时完成，大的Job可能一次性无法获取到所需的所有资源，但也不至于饿死！

四、推测执行
	在MR运行期间，MRAppMaster会为最慢的任务启动一个备份任务，
	备份任务和当前任务先运行完的会采用其结果！

	不适用场景：
	①数据有倾斜
	②特殊场景：向数据库写记录

	推测执行是典型的以空间换时间，因此在集群资源不足时，如果开启了推测执行，反而不能取得效果！

五、Hadoop的优化
1.小文件的优化
	①源头上处理，在上传到集群之前，提前处理小文件
	②小文件已经在HDFS存在，可以使用hadoop archieve进行归档
	③在运行MR时，可以使用CombineTextInputFormat将多个小文件规划到一个切片中
	④小文件过多，可以开启JVM重用
		
		
		
2.MR的优化
	核心：  ①合理设置MapTask和ReduceTask的数量
				在合理利用资源的情况下，提高程序并行运行的效率
			②避免数据倾斜
				Map端的数据倾斜：在切片时，注意每片数据尽量均匀。
					防止有些不可切片的数据！
				Reduce端的数据倾斜：提前对数据进行抽样调查，统计出大致的分布范围，
					根据分布范围，合理编写Partitioner，让每个分区的数据尽量均衡
							
			③优化磁盘IO和网络IO
				a)启用combiner
				b)启动压缩
				c)调大MapTask缓冲区的大小，减少溢写次数
				d)调大MapTask中merge阶段一次合并的片段数，减少合并花费的时间
				e)调大reduceTask中shuffle线程可以使用的内存，减少溢写次数
				d)调大reduceTask中，input.buffer的大小，提前缓存部分数据到buffer中
					






	


		
		
		







				