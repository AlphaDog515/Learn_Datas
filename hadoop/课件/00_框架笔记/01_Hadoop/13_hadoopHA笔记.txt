一、Zookeeper的安装
	1. 简介
			Zookeeper是java编写的一个开源的分布式的存储中间件！
			Zookeeper可以用来存储分布式系统中各个进程都关心的核心数据！
			Zookeeper采取观察者模式设计，可以运行客户端在读取数据时，
				设置一个观察者一旦观察的节点触发了指定的事件，
				服务端会通知客户端线程，客户端可以执行回调方法，执行对应的操作！
				
			Zookeeper=文件系统+通知机制
		
	2. 数据结构
			在Zookeeper中每个存储数据的基本单位称为znode，
			每个znode都有一个路径标识，还可以保存byte[]类型的数据，
			这个数据默认为1m。
			
			所有的znode都挂载在/节点上
			
	3. 安装
			①必须保证环境变量有JAVA_HOME
			②解压，后配置conf/zoo.cfg文件，配置dataDir=非/tmp目录即可
			③集群模式，需要将集群中所有的zk实例进行配置
			④如果是集群模式，需要在dataDir中，配置myid文件，myid中需要编写zk的serverid
			
	4. 使用
			启动：  bin/zkServer.sh start
					bin/zkCli.sh -server host:port 命令
					 
			停止：  bin/zkServer.sh stop
					bin/zkCli.sh -server host:port  close|exit
					  
	5. 常用的命令
			增：    create [-s] [-e]  path  data
						-s:  创建一个带序号的znode
						-e:  创建一个临时znode，临时的znode被所创建的session拥有，
								一旦session关闭，临时节点会被删除
			
			删：   delete  path 
					rmr    path
			
			改：   set  path  data
			
			查：   get  path 
				   stat path
				   ls   path
				   ls2  path
				

				
	6. 支持四字命令
			nc hadoop101 2181  ruok
			
	
	7. 设置观察者
			get  path watch：  监听指定节点数据的变化
			ls   path watch：  监听当前路径子阶段数据的变化，一旦新增或删除了子节点，会触发事件
			
			注意： 观察者在设置后，只有当次有效！
	


	
二、ZK集群的注意事项
1. ZK在设计时，采用了paxos协议设计，这个协议要求，集群半数以上服务实例存储，
		集群才可以正常提供服务
		
2. ZK集群中，server有Leader 和Followwer两种角色
		Leader只有一个，在集群启动时，自动选举产生！
		
		选举Leader时，只有数据和Leader保持同步的Follower有权参与竞选Leader!
		在竞选Leader时，serverid大的Server有优势！
		
		
		
3. 集群模式ZK的写流程
		①客户端可以连接任意的zkserver实例，向server发送写请求命令
		
		②如果当前连接的server不是Leader，server会将写命令发送给Leader
		
		③Leader将写操作命令广播到集群的其他节点，所有节点都执行写操作命令
		
		④一旦集群中半数以上的节点写数据成功，Leader会响应当前Server，让当前Server
			响应客户端，写操作完成！
				

		

一、Hadoop的HA
1. HA
		H(high)A(avilable)： 高可用，意味着必须有容错机制，不能因为集群故障导致不可用！
		
		HDFS：满足高可用
					NN：  一个集群只有一个，负责接受客户端请求！
					DN：  一个集群可以启动N个
		
		YARN：满足高可用
					RM：  一个集群只有一个，负责接受客户端请求！
					NM：  一个集群可以启动N个
					
		
		实现hadoop的HA，必须保证在NN和RM故障时，采取容错机制，可以让集群继续使用！
	

	
2. 防止故障
	核心： 避免NN和RM单点故障
	
	以HDFS的HA为例：
		①NN启动多个进程，一旦当前正在提供服务的NN故障了，让其他的备用的NN继续顶上
		
		②NN负责接受客户端的请求
			在接收客户端的写请求时，NN还负责记录用户上传文件的元数据
			
			保证： 正在提供服务的NN，必须和备用的NN之中的元数据必须是一致的！
			
			元数据的同步：  ①在active的nn格式化后，将空白的fsimage文件拷贝到所有的nn的机器上
							②active的nn在启动后，将edits文件中的内容发送给Journalnode进程
								standby状态的nn主动从Journalnode进程拷贝数据，保证元数据的同步
							  
			注意：  ①Journalnode在设计时，采用paxos协议, Journalnode适合在奇数台机器上启动！
						在hadoop中，要求至少需要3个Journalnode进程
					
					②如果开启了hdfs的ha,不能再启动2nn
				
				
		③当启动了多个NN时，是否允许多个NN同时提供服务？
				不允许多个NN同时对外提供服务，因为如果多个NN同时对外提供服务，那么
				在同步元数据时，非常消耗性能，而且容易出错！
				
				在同一时刻，最多只能有一个NN作为主节点，对外提供服务！
				其余的NN，作为备用节点！
				
				使用active状态来标记主节点，使用standby状态标记备用节点！

				
				
3. HDFS HA的搭建步骤
	①配置
			①fs.defaultFS=hdfs://hadoop101:9000 进行修改
			
			②在整个集群中需要启动N个NN，配置N个NN运行的主机和开放的端口！
			
			③配置Journalnode
			
	②启动
			①先启动Journalnode
			②格式化NN，将格式化后的fsimage文件同步到其他的NN
			③启动所有的NN，需要将其中之一转为active状态
		


		
一、压缩
	1.压缩的目的
		压缩的目的是在MR运行期间，提高MR运行的效率！
		压缩可以减少MR运行期间的磁盘IO和网络IO！
			
	2.压缩的原则
		IO密集型，多用压缩！
		计算密集型，CPU负载过重，少用压缩！
			
	3.Hadoop支持的压缩格式
	默认：deflate,bzip2,gzip
	额外安装的：lzo,snappy

	特点：  bzip2压缩比最高，压缩速度最慢
			snappy压缩速度最快，压缩比凑合
			deflate，gzip 折中
			
	使用便利性：LZO压缩格式最麻烦！
					①额外安装LZO压缩格式
					②如果JOB输入目录中的文件为LZO压缩格式，需要为每个文件创建索引
						如果不创建索引，那么输入的文件无法切片，整个文件作为1片
						还需要使用LZO特定的输入格式，使用LZOInputFormat！
						 
				其他的压缩格式，和纯文本文件使用一致的，不需要额外设置！
				
	可切片的角度：  
			如果Job的输入采用了以下压缩格式，只有以下格式支持切片！
			只有bzip2和lzo可以切片！
				 
	使用场景：
		Bzip2： 对速度没有要求，常作为reduce输出结果的压缩格式！
				即便job运行后，输出的结果还需要被另一个Job继续处理，Bzip2格式也可以支持切片！
				
		Lzo:    作为Job输入文件的压缩格式！
		
		Snappy: 作为shuffle阶段的压缩格式！
		
		
		Mapper 运算结束后，需要向磁盘溢写，500M的数据，没有用压缩之前，写的速度100M/s
		采用了Snappy压缩，需要向磁盘溢写，500M的数据，采用了snappy压缩，写的速度100M/s，500M--->300M
		 
		Reduce拷贝300M的数据----> 解压缩（速度很快，解压缩消耗的时间可以忽略不计）------>
		
		
	压缩的考虑：
	①Mapper的输入： 主要考虑每个文件的大小，如果文件过大，需要使用可以切片的压缩格式！
	②Reducer的输出：reducer的输出主要考虑，输出之后，是否需要下一个Job继续处理！
						单个reducer输出的结果的大小！
						如果需要被下个Job继续处理，且单个文件过大，也要使用可以切片的压缩格式！
						
	③shuffle阶段：  速度快即可

	
	压缩的参数：
		io.compression.codecs： 代表整个Job运行期间，可以使用哪些压缩格式！
								配置这个参数后，配置的压缩格式会被自动初始化！
								默认值： deflate,gzip,bzip2
		
		mapreduce.map.output.compress: map阶段输出的key-value是否采用压缩
								默认值： false
		
		mapreduce.map.output.compress.codec： map阶段输出的key-value采用何种压缩
								默认值： deflate
		
		
		mapreduce.output.fileoutputformat.compress： job在reduce阶段最终的输出是否采用压缩
								默认值： false
		
		mapreduce.output.fileoutputformat.compress.codec：job在reduce阶段最终的输出采用何种压缩
								默认值： deflate
								
								
		mapreduce.output.fileoutputformat.compress.type: 如果Job输出的文件以SequenceFile格式！
								SequenceFile中的数据，要以何种形式进行压缩！
								NONE： 是否压缩及如何压缩取决于操作系统
								RECORD(默认)：每个key-value对作为一个单位，压缩一次
								BLOCK： SequenceFile中的block，SequenceFile中的block默认为64K,
											每个block压缩一次！


	
二、调度器
1. FIFO调度器
		FIFO调度器的特点就是单队列，所有的Job按照客户端提交的先后顺序，先到先服务！
		
		弊端：  如果当前队列中有一个大的Job，非常消耗资源，
				那么这个Job之后的其他Job都需要付额外的等待时间！
				造成集群的资源利用率不足！
				
		解决：  采取多队列的配置
		
		
2. 容量调度器
		容量调度器的本质是多个FIFO的队列组成！
		
		Hadoop默认使用就是容量调度器！
		
		特点： 容量
				①每个队列可以配置一定的容量，空闲的资源可以匀给其他队列临时使用
				②可以配置每个job使用的容量的限制，防止一个大的job独占所有资源
				③可以配置每个用户可以使用的容量限制，防止当个用户占用所有资源
					
		优点：  ①配置灵活，及时刷新即可，不需要重新启动集群
				②资源利用率高
				③安全，可以配置每个队列的访问用户限制
		
		
3. 公平调度器
		公平调度器的设置和容量调度器大致相同，也是多条队列，每天队列都可以设置一定的容量！
		每个Job，用户可以设置容量！
		
		区别：  公平调度器在调度策略上，采用最大最小公平算法，来调度Job，这个算法会保证
					同一个队列中，所有已经提交，未运行结束的Job，获取到队列中的资源是平等的！
					
				导致在一个队列中，小的Job运行有优势，
				大的Job可能不能及时获取到必须的所有资源，但是不至于饿死！
				
		当前队列A ： 目前有20个CPU，20G 内存...资源
		
		每个Job理论上应该分配  5个CPU， 5G内存，在实际分配资源时，只考虑内存！
		
		队列A中已经提交，未运行的Job：
		
		job1:   2 个MapTask      2 CPU,2G 内存
				2 个ReduceTask   1 CPU,1G 内存
				
		job2：  2 个MapTask      4 CPU,2G 内存
				2 个ReduceTask   2 CPU,2G 内存
				
		job3:   1 个MapTask      1 CPU,1G 内存
				1 个ReduceTask   1 CPU,1G 内存 
				
		job4:   4 个MapTask      4 CPU,2G 内存
				4 个ReduceTask   2 CPU,2G 内存
		