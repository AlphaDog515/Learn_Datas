复习
一、读流程
 找到所读数据的Regionserver
	①客户端请求zk的 /hbase/meta-region-server节点，读取到hbase:meta表所在regionserver
	②请求rs，下载meta表
	③查询meta表，当前所读region所在的rs
	④向rs发起读请求
 
 读数据
	⑤初始化两种scanner（扫描器）
		memstorescannner：负责扫描store的memstore区域
		storefileScanner: 负责扫描store的若干storefile
			
	⑥如果扫了storefile，会将storefile数据所在的block(HFile中的block，64k)缓存到blockcache中
		blockcache是rs的读缓存，默认占用rs所在堆的40%容量，采取LRU回收策略进行缓存的回收！
		
		此后如果读取的storefile文件已经缓存在blockcache了，那么就会从blockcache中读取数据
		而无需扫描磁盘读取storefile。
		
	⑦扫描memstore,storefile,blockcache,取出version（时间戳最大的数据）返回给客户端！
	
二、Flush
1.将memstore中的数据，刷写到磁盘，生成storefile的过程称为flush!
	flush的目的是在memstore中对数据的rowkey进行排序，排序后刷写到磁盘后的数据
	是有序的，方便检索！
		
2. 实现flush
	hbase shell:  flush '表名'| 'region名'
		
3. 自动flush
	①从容量上来说
		a)如果单个memstore使用的容量超过hbase.hregion.memstore.flush.size(默认128M)，
		整个region的memstore都会刷写！
			
		在刷写时，客户端依然可以向rs发起写请求，写的数据依然是存储在memstore中，但是
		如果memstore使用的容量超过hbase.hregion.memstore.flush.size(默认128M) *
		hbase.hregion.memstore.block.multiplier（默认值4）,此时memstore会自动block（阻塞）,
		客户端无法再执行写入！
			
		b)整个rs所有的memstore使用的容量总和超过java_heapsize  // 堆大小，时间，wal数量
		*hbase.regionserver.global.memstore.size（默认值0.4）
		*hbase.regionserver.global.memstore.size.lower.limit（默认值0.95）
		整个rs所有的memstore会依次按照大小顺序刷写。
			
		在刷写时，如果rs所有的memstore使用的容量总和超过java_heapsize
		*hbase.regionserver.global.memstore.size（默认值0.4），此时所有的memstore也会block
			
		②从时间上来说
			每间隔hbase.regionserver.optionalcacheflushinterval（1h），自动flush
			
		③从WAL正在写入的日志数量上来说
			如果有大量的正在使用的WAL日志，说明memstore中有大量尚未刷写的数据，一旦数据过多，
			rs进程崩溃，此时恢复数据时间过长。所以，一旦正在使用的WAL日志文件的数量超过
			hbase.regionserver.max.logs(32)，此时，会根据WAL中记录的日志的先后顺序依次刷写memstore!

三、 compact
	目的：对store中的多个hfile文件定期合并，消除每次flush产生的大量的小文件。
		对hfile中无效的过期的数据，进行合并整理，较少数据量！
			
	分类：minor_compact: 将临近的多个小文件，合并为一个大文件。不会删除delete类型的数据(0.94之前)！
		major_compact： 将store目录中所有的文件，合并为一个大文件，会删除delete类型的数据！
			
		minor_compact和major_compact的区别是minor_compact只能合并有限数量的hfile！
		major_compact是合并目录下所有的文件！
			
	对于major_compact建议取消自动合并的设置，改为在集群空闲时，手动执行合并！
		
四、region的切分
	每张表在创建时，只有一个region.随着这个region中写入数据越来越多，此时，region
	会自动完成切分，切分后的region有可能出于负载均衡的目的，会分配给其他的rs负责！
	
	自动切分：
		①统计当前regionserver中负责的 当前表的region个数，称为tableRegionCount
		
		②0=tableRegionCount 或 tableRegionCount>100，此时某个region超过hbase.hregion.max.filesize(10G)
		时切分（一分为二）！
		
		③0<tableRegionCount<=100,此时使用
			initialSize * tableRegionsCount * tableRegionsCount * tableRegionsCount
			和 hbase.hregion.max.filesize(10G)进行对比取较小值
				
		④initialSize，取决于用户的配置，由hbase.increasing.policy.initial.size自定义！
			如果没有自定义通常为 2*hbase.hregion.memstore.flush.size,为 256M
				
五、API
1.常用的API
	Connection:  代表客户端和集群的一次连接。Connection的创建是重量级的，是线程安全的！
				因此可以在多个线程中共享！
				建议一个应用只创建一个Connection！
					
				ConnectionFactory.getConnnection();
					
	Admin：     对hbase执行管理性命令的客户端对象！
				对库的创建，查看，删除等
				对表的创建，查询，删除等！
				
				Admin的创建是轻量级的，不是线程安全的！建议每个线程都有自己的Admin对象！
				Connection.getAdmin()
				
	Table：     对表中的数据执行增删改查！
				
				Table创建是轻量级的，不是线程安全的！建议每个线程都有自己的Table对象！
				
				Connection.getTable(TableName tn);
				
	TableName:  代表表的名称
	
	HTableDescriptor: 代表表的定义和描述！在这个对象中可以对表中的列族进行设置！
	
	HColumnDescriptor: 代表列族的定义和描述！
	
	NameSpaceDescriptor:  名称空间的定义和描述！
	
	Put:  对单行数据执行put操作的对象！在put中可以定义每次单行插入的数据！
	
	Get： 对单行数据的查询操作的对象！ 在get中可以定义每次查询的参数！
	
	Result: 单行查询返回的结果集！在result中包含若干个cell
	
	工具类：   CellUtil.cloneXxx(): 将cell中的某个数据克隆为byte[]
			   Bytes.toXxx():  		从byte[]转为常用的基本数据类型
			   Bytes.toBytes():  	将常用的基本数据类型转为byte[]
			




一、MR和HBase集成

1.Hbase可以做简单的查询，但是无法对查询的结果进行深加工！
	可以使用MR来进行hbase中数据的深加工！
		
2. MR必须持有可以读取HBase中数据的api才可以！
	在MR启动时，在MR程序的类路径下，把读取hbase的jar包加入进去！
	
	①使用MR读取hbase，需要哪些jar包？
		通过执行hbase mapredcp查看
		
	②如何让MR在运行时，提前将这些jar包加入到MR的环境中？
		hadoop jar MRjar包 主类名  参数
		
		hadoop命令一执行，先去读取hadoop-config.sh(hadoop环境的配置脚本，用来配置环境变量)
		hadoop-config.sh读取hadoop-env.sh(建议将hadoop运行的变量配置在此脚本中)
		
		在hadoop-env.sh 44行后，添加：
		export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/opt/module/hbase/lib/*
	
	
3. 测试官方案例
	①hadoop jar hbase-server-1.3.1.jar CellCounter t1 /hbasemr/cellcount ,
	
	②hadoop jar hbase-server-1.3.1.jar rowcounter t1
	
	③向hbase中导入数据，需要手动建表，需要把数据上传到HDFS，注意数据中字段的顺序要和
	  -Dimporttsv.columns的顺序一致	  
		hadoop jar hbase-server-1.3.1.jar importtsv
		-Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:age,info:gender t2 /hbaseimport

		HBASE_ROW_KEY： 代表rowkey列
		
	在示例程序中添加参数，在core-site.xml中添加！
			


二、
	HBase负责存储，Hive负责分析！
	hive的本质是使用HQL语句，翻译为MR程序，进行计算和分析！

①环境配置
	让hive持有可以读写hbase的jar包
		在HIVE_HOME/lib/下，将操作hbase的jar包以软连接的形式，持有！
	
	修改hive-site.xml添加zookeeper配置
			
			
②在hive中建表，这个表需要和hbase中的数据进行映射; 
	a) 	数据已经在hbase中，只需要在hive建表，查询即可，只能创建external non-native table
		create external table hbase_t3(
		id int,
		age int,
		gender string,
		name string)
		STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
		WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,info:age,info:gender,info:name")
		TBLPROPERTIES ("hbase.table.name" = "t3");
		注释：hbase中表t3对应hive中的hbase_t3，key是第一列；
	
	b) 	数据还尚未插入到hbase，可以在hive中建表，建表后，在hive中执行数据的导入
		将数据导入到hbase，再分析。表必须是managed non-native table！
		①建表
			CREATE  TABLE `hbase_emp`(
			  `empno` int, 
			  `ename` string, 
			  `job` string, 
			  `mgr` int, 
			  `hiredate` string, 
			  `sal` double, 
			  `comm` double, 
			  `deptno` int)
			STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
			
			WITH SERDEPROPERTIES (
				"hbase.columns.mapping" = 
				":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno")		
				TBLPROPERTIES ("hbase.table.name" = "employee"); // rowkey不能为空
				// 不需要在hbase中创建表,不能写row format，这是文本格式
		
		②替换hive-hbase-handler.jar // hbase与hive，jar不合适
		
		③使用insert向表中导入数据
			insert into table hbase_emp select * from emp
		
		
		
		show create table emp;
		CREATE EXTERNAL TABLE `emp`(
		`empno` int, `ename` string,`job` string,`mgr` int, 
		`hiredate` string, `sal` double, `comm` double, `deptno` int)
		
		ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' 
		
		STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' 
		OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
		
		LOCATION 'hdfs://hadoop102:9000/hive/warehouse/emp'
		
		TBLPROPERTIES ('COLUMN_STATS_ACCURATE'='true', 
		'numFiles'='1', 'totalSize'='657', 'transient_lastDdlTime'='1578488920')
		
		ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe' 
			
		
④注意事项：
	a)	在建表时，hive中的表字段的类型要和hbase中表列的类型一致,以避免类型转换失败
		造成数据丢失
	b)	row format 的作用是指定表在读取数据时，使用什么分隔符来切割数据，
		只有正确的分隔符，才能正确切分字段
	
	c)	管理表：managed_table
			由hive的表管理数据的生命周期！在hive中，执行droptable时，
			不禁将hive中表的元数据删除，还把表目录中的数据删除
		外部表： external_table
			hive表不负责数据的生命周期！在hive中，执行droptable时，
			只会将hive中表的元数据删除，不把表目录中的数据删除
					
					
二、hive集成hbase理论

1.Storage Handlers
	Storage Handlers是一个扩展模块，帮助hive分析不在hdfs存储的数据！
		例如数据存储在hbase上，可以使用hive提供的对hbase的Storage Handlers，
		来读写hbase中的数据！		
	
	native table: 本地表！ hive无需通过Storage Handlers就能访问的表。
		例如之前创建的表，都是native table！
	
	non-native table : hive必须通过Storage Handlers才能访问的表！
		例如和hbase集成的表！
	
	【管理表外部表与本地表与非本地表四种组合】
				
2. 在建表时
	创建native表：
		[ROW FORMAT row_format] [STORED AS file_format]
			file_format: ORC|TEXTFILE|SEQUNCEFILE|PARQUET
			都是hive中支持的文件格式，由hive负责数据的读写！
		
	或创建non-native表:
		STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)]
											   [with serdeproperties]
		数据在外部存储【不在hdfs上】，hive通过Storage Handlers来读写数据！
		
		
3. SERDE:
		序列化器和反序列化器
		
		表中的数据是什么样的格式，就必须使用什么样的SerDe!
			纯文本：  	row format delimited ，默认使用LazySimpleSerDe
			JSON格式：  使用JsonSerde
			ORC：    	使用读取ORC的SerDe
			Paquet:  	使用读取PaquetSerDe
		
		普通的文件数据，以及在建表时，如果不指定serde，默认使用LazySimpleSerDe！
		
		例如： 数据中全部是JSON格式 jsondata
			{"name":"songsong","friends":["bingbing","lili"]}
			{"name":"songsong1","friends": ["bingbing1" , "lili1"]}

		错误写法：
			create table testSerde(
			name string,
			friends array<string>
			)
			row format delimited fields terminated by ','
			collection items terminated by ','
			lines terminated by '\n';

		如果指定了row format delimited ,此时默认使用LazySimpleSerDe！
			LazySimpleSerDe只能处理有分隔符的普通文本！

		现在数据是JSON，格式{},只能用JSONSerDE
		
		添加jar包：
		add jar /opt/module/hive/hcatalog/share/hcatalog/hive-hcatalog-core-1.2.1.jar
		
		create table testSerde2(
			name string,friends array<string>)
			ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe' 
			STORED AS TEXTFILE // 创建本地表

		
		