一、准备工作：
	添加硬盘：
		(1)关闭虚拟机，编辑虚拟机设置，硬件，硬盘，添加，硬盘，SCSI，
			下一步，文件路径选择VMware一致，确定；
		(2)开机，分区：df -h查看分区，fdisk -l查看磁盘情况，执行分区命令：fdisk /dev/sdb；
			依次选择：m n p 1 1 回车 w，至此分区完毕；
		(3)格式化分区：mkfs -t ext4 /dev/sdb1；
		(4)挂载：创建/newdisk目录，mount /dev/sdb1 /newdisk，使用df -h查看；
		(5)永久挂载：vim /etc/fstab复制一行修改，生成uuid：blkid /dev/sdb1；
		(6)立即生效：mount -a,查看：df -h；

	设置共享目录：编辑虚拟机设置，选择选项，共享文件夹，启用，确定；
	内存设置：编辑虚拟机设置，内存，调节，确定；


二、Hadoop集群搭建
克隆虚拟机：
(1)克隆三台虚拟机【ctrl++放大终端】	 
	修改主机名【/etc/sysconfig/network】；
	修改虚拟机网卡【/etc/udev/rules.d/70-persistent-net.rules】；
	修改IP【/etc/sysconfig/network-scripts/ifcfg-eth0】；
	
(2)创建用户atguigu用户，并设置密码，修改sudoers配置文件，为atguigu用户设置root权限;
(3)etc/hosts配置所有集群节点主机映射，修改windows本地映射【C:\Windows\System32\drivers\etc】；
(4)三台虚拟机相互使用主机名ping，以及ping www.baidu.com;
	ping hadoop102 -c 3 // ping三次以后停止ping；
(5)系统时间日期，在网络上同步，点击系统，管理，时间日期，添加一个ntp服务器；


安装JDK以及Hadoop
(1)选择一台机器创建/opt/module和/opt/soft目录，改变所有者为atguigu;
(2)在第二台机器上安装JDK，Hadoop，解压、配置环境变量；
(3)注意执行:source /etc/profile;
	JAVA_HOME=/opt/module/jdk1.8.0_121  
	HADOOP_HOME=/opt/module/hadoop-2.7.2
	PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
	export JAVA_HOME PATH HADOOP_HOME

ssh免密登录：
(1)【.ssh/】ssh-keygen -t rsa 生成公钥私钥，rsa按照什么算法生成；
	实现a机器a用户与b机器b用户建立信任；
(2)将公钥拷贝到要免密登录的目标机器上:【.ssh/】ssh-copy-id atguigu@hadoop102  // 本机也需要执行；
(3)使用ssh在集群中执行命令的时候有login与nologin之分，在/home/atguigu下修改隐藏文件：
	.bashrc，所有节点末尾加上source /etc/profile；


编写两个脚本（同步脚本，命令执行脚本）
(1)同步脚本实现不同节点之间文件的安全拷贝；
(2)命令执行脚本保证当前节点执行的命令，在集群的其他节点也执行；
(3)将脚本移动到第二台机器：/home/atguigu/bin下，先创建bin，并且将文件改为具有执行权限；
// 按照$PATH环境变量定义的目录查找顺序搜到的第一个命令；
// 【shell内建命令，$PATH所列目录下的外部命令】

修改配置文件
(1)依次在第二台机器上修改core,mapred,yarn配置文件；
(2)修改日志的配置：在第二个节点修改mapred,yarn配置文件；
(2)执行分发脚本，在集群上分发文件【/opt/module /opt/soft】；
(3)日志查看方式：在yarn上点击history，点击log；


启动hadoop：
在namenode所在节点格式化namenode：hdfs namenode -format；
所有节点启动datanmode；
secondarynamenode所在节点启动；
MR所在节点启动；
所有节点启动namemanager;


时间同步（非必须）
编写定时任务 ntpdate ntp 授时服务器；
ntpdate -u ntp1.aliyun.com


群起集群
(1)在/opt/module/hadoop-2.7.2/etc/hadoop下配置slaves，将集群节点hostname放进去，
	一个主机一行，不加空格，其他字符；
(2)在哪个节点上启动集群就在那个集群上配置slaves，不需要分发；
(3)在MR节点执行stop-all.sh，后执行start-all.sh;
	在存在ssh的前提下，任意节点可以整体启动/停止HDFS start-dfs.sh   /  stop-dfs.sh
	整体启动/停止YARN，只能在RM节点：start-yarn.sh  /  stop-yarn.sh
(4)在配置文件指定的节点启动：sbin/mr-jobhistory-daemon.sh start historyserve；



core-site.xml
<configuration>
	<!-- 指定HDFS中NameNode的地址 -->
	<property>
		<name>fs.defaultFS</name>
      	<value>hdfs://hadoop102:9000</value>
	</property>

	<!-- 指定Hadoop运行时产生文件的存储目录 -->
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/opt/module/hadoop-2.7.2/data/tmp</value>
	</property>
</configuration>

hdfs-site.xml
<configuration>
	<property>
			<name>dfs.replication</name>
			<value>3</value>
	</property>

	<!-- 指定Hadoop辅助名称节点主机配置 -->
	<property>
       	<name>dfs.namenode.secondary.http-address</name>
      	<value>hadoop104:50090</value>
	</property>
</configuration>


mapred-site.xml
<configuration>
	<!-- 指定mr运行在yarn上 -->
	<property>
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
	</property>
	
	<property>
        <name>mapreduce.jobhistory.address</name>
        <value>hadoop102:10020</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>hadoop102:19888</value>
    </property>
    <!--第三方框架使用yarn计算的日志聚集功能 -->
    <property>
        <name>yarn.log.server.url</name>
        <value>http://hadoop102:19888/jobhistory/logs</value>
    </property>
</configuration>


yarn-site.xml
<configuration>
	<!-- Site specific YARN configuration properties -->
	<!-- reducer获取数据的方式 -->
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>

	<!-- 指定YARN的ResourceManager的地址 -->
	<property>
		<name>yarn.resourcemanager.hostname</name>
		<value>hadoop103</value>
	</property>

	<!-- 日志聚集功能 -->
	<property>
		<name>yarn.log-aggregation-enable</name>
		<value>true</value>
	</property>

	<!-- 日志保留时间设置7天 -->
	<property>
		<name>yarn.log-aggregation.retain-seconds</name>
		<value>604800</value>
	</property>
</configuration>

/opt/module/hadoop-2.7.2/etc/hadoop/slaves
hadoop102
hadoop103
hadoop104


	




三、zookeeper的安装
(1)解压tar包到指定目录；
(2)在解压目录下创建datas文件夹；
(3)重命名conf下的zoo_sample.cfg为zoo.cfg；
(4)在zoo.cfg中修改：dataDir=/opt/module/zookeeper-3.4.10/datas
	空白处添加：
	server.102=hadoop102:2888:3888
	server.103=hadoop103:2888:3888
	server.104=hadoop104:2888:3888
(5)在/opt/module/zookeeper-3.4.10/datas下创建一个myid的文件：
	编辑myid文件，在文件中添加与server对应的编号：如102，
	分发文件并修改myid为：103，104；
(6)启动：bin/zkServer.sh start；

四、flume的安装
解压到指定目录并配置环境变量分发即可；

五、Kafka的安装
(1)解压到指定目录并配置环境变量；
(2)安装目录下创建logs文件夹，并修改config文件夹下的server.properties:
	#broker的全局唯一编号，不能重复，依次修改
	broker.id=102	
	#删除topic功能使能
	delete.topic.enable=true	
	#kafka运行日志存放的路径
	log.dirs=/opt/module/kafka/logs	
	#配置连接Zookeeper集群地址
	zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181
(3)分发即可；
(4)启动：bin/kafka-server-start.sh -daemon config/server.properties
   停止：bin/kafka-server-stop.sh stop

kafka-manager的配置：
	(1)上传压缩包kafka-manager-1.3.3.15.zip到集群;
	(2)解压：unzip /opt/soft/kafka-manager-1.3.3.15.zip -d /opt/module
	(3)修改配置文件:conf/application.conf
		kafka-manager.zkhosts="kafka-manager-zookeeper:2181"
		修改为：kafka-manager.zkhosts="hadoop102:2181,hadoop103:2181,hadoop104:2181"
	(4)修改文件的权限：chmod 777 kafka-manager
	(5)启动：bin/kafka-manager;
		后台运行：bin/kafka-manager > start.log 2>&1 &;
	(6)登录hadoop103:9000页面查看详细信息;
	注意：namenode端口也是9000，所以不能在hadoop102上运行kafka-manager ;

Kafka Monitor的配置：
	消费者偏移量监测，记录每次消费的位置；
	(1)上传jar包KafkaOffsetMonitor-assembly-0.4.6-SNAPSHOT.jar到集群;
	(2)在/opt/module/下创建kafkamonitor文件夹,并放入jar包;	
	(3)在/opt/module/kafkamonitor目录下创建启动脚本start.sh，内容如下：
		#!/bin/bash
		java -cp KafkaOffsetMonitor-assembly-0.4.6-SNAPSHOT.jar \
		com.quantifind.kafka.offsetapp.OffsetGetterWeb \
		--offsetStorage kafka \
		--kafkaBrokers hadoop102:9092,hadoop103:9092,hadoop104:9092 \
		--kafkaSecurityProtocol PLAINTEXT \
		--zk hadoop102:2181,hadoop103:2181,hadoop104:2181 \
		--port 8086 \
		--refresh 10.seconds \
		--retain 2.days \
		--dbName offsetapp_kafka &	
	(4)启动KafkaMonitor: ./start.sh;
	(5)登录页面hadoop103:8086端口查看详情;


	






